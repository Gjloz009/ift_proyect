{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c205ba-6223-440e-817d-edacdae61e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moduos\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695e495-0623-4e6e-b7c9-a4393b51a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Funciones\n",
    "\n",
    "def get_encoding(filename_in: str, filename_out:str) -> None:\n",
    "    import chardet\n",
    "\n",
    "    with open(filename_in, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "\n",
    "        try:\n",
    "            decoded_data = raw_data.decode(encoding=encoding)\n",
    "        except UnicodeDecodeError:\n",
    "            # Si falla, intenta con una lista de codificaciones comunes\n",
    "            common_encodings = ['utf-8', 'iso-8859-1', 'cp1252']\n",
    "            for enc in common_encodings:\n",
    "                try:\n",
    "                    decoded_data = raw_data.decode(encoding=enc)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            else:\n",
    "                raise UnicodeDecodeError(f\"No se pudo decodificar el archivo con las codificaciones probadas. Último intento fallido con {encoding}\")\n",
    "\n",
    "    with open(filename_out, 'w', encoding='utf-8') as f2:\n",
    "        f2.write(decoded_data)\n",
    "\n",
    "def tweak_df(df: pl.DataFrame, columns_trans: list, schema: dict) -> pl.DataFrame:\n",
    "  # aplicación de funciones para el df\n",
    "  # acepta lista de expresiones\n",
    "  return df.with_columns(columns_trans).cast(schema)\n",
    "\n",
    "def transformations(file_name: str,tweak_list: list,schema_dict: dict) -> pl.DataFrame:\n",
    "  # decodificando\n",
    "  get_encoding(f\"{file_name}.csv\",f\"{file_name}_decoded.csv\")\n",
    "  # leyendo el archivo\n",
    "  df = pl.read_csv(f\"{file_name}_decoded.csv\")\n",
    "  # aplicando los cambios al df\n",
    "  return tweak_df(df,tweak_list,schema_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b093d0-3c68-4e3f-9b53-21628a32c0f7",
   "metadata": {},
   "source": [
    "Nos metimos al IFT (instituto federal de telecomunicaciones) en especial a la sección de servicios móvil de acceso a internet, para delimitar\n",
    "\n",
    "Subsecciones:\n",
    "  - Lineas (Ya)\n",
    "  - Lineas (Serie hitórica desde 2010) (Ya)\n",
    "  - Tráfico de datos (TB) (Ya)\n",
    "  - Índice de concentración (IHH) (Ya)\n",
    "  - Participación de mercado (Serie desde 2010) (Ya)\n",
    "  - Líneas por cada 100  habitantes (Estatal) (No)\n",
    "  -Líneas por cada 100 habitantes (Serie historica desde 2010) (No)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247ffcf-b707-4bdc-a3e0-019cbe0bc167",
   "metadata": {},
   "source": [
    "## LINEAS\n",
    "Tabla de información de las líneas de servicio móvil de acceso a internet por esquema de pago (prepago y pospago), serie mensual desde el 2013.\n",
    "\n",
    "[diccionario de datos](https://docs.google.com/spreadsheets/d/176qSChhhpF43hzslsFscsAcBblJ0Wa_cyBHnzWTq1Eg/edit?gid=0#gid=0)\n",
    "\n",
    "### To do:\n",
    "- eda\n",
    "- encontrar los valores distintos\n",
    "\n",
    "### Done:\n",
    "- Ir de acuerdo al esquema en el diccionario de datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87524409-f380-42de-866d-913c48dfd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero vamos a ver lineas\n",
    "#nos falta un catálogo de datos\n",
    "!wget https://bit.ift.org.mx/descargas/datos/tabs/TD_LINEAS_INTMOVIL_ITE_VA.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547cc00-6bcc-46d0-a0e5-02848c5cfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación de esquema de acuera al diccionario de datos\n",
    "schema_lineas = {\n",
    "    \"FECHA\": pl.Date,\n",
    "    \"ANIO\": pl.Int16,\n",
    "    \"MES\": pl.Int8,\n",
    "    # \"K_GRUPO\": pl.Categorical,\n",
    "    \"GRUPO\": pl.Categorical,\n",
    "    \"K_EMPRESA\": pl.Categorical,\n",
    "    \"EMPRESA\": pl.Categorical,\n",
    "    \"CONCESIONARIO\": pl.Categorical,\n",
    "    \"L_PREPAGO_E\": pl.Int32,\n",
    "    \"L_POSPAGO_E\": pl.Int32,\n",
    "    \"L_POSPAGOC_E\": pl.Int32,\n",
    "    \"L_POSPAGOL_E\": pl.Int32,\n",
    "    \"L_NO_ESPECIFICADO_E\": pl.Int32,\n",
    "    \"L_TOTAL_E\": pl.Int32,\n",
    "    \"FOLIO\": pl.Categorical\n",
    "  }\n",
    "\n",
    "tweak_columns = [\n",
    "    pl.col( \"L_PREPAGO_E\" ).str.replace_all(\",\",\"\").str.to_integer(base=10),\n",
    "    pl.col( \"L_POSPAGO_E\" ).str.replace_all(\",\",\"\").str.to_integer(base=10),\n",
    "    pl.col( \"L_POSPAGOC_E\" ).str.replace_all(\",\",\"\").str.to_integer(base=10),\n",
    "    pl.col( \"L_POSPAGOL_E\" ).str.replace_all(\",\",\"\").str.to_integer(base=10),\n",
    "    pl.col( \"L_NO_ESPECIFICADO_E\" ).str.replace_all(\",\",\"\").str.to_integer(base=10),\n",
    "    pl.col( \"L_TOTAL_E\" ).str.replace_all(\",\",\"\").str.to_integer(base=10),\n",
    "    pl.col(\"FECHA\").str.to_date(\"%d%b%Y\"),\n",
    "    pl.col(\"FOLIO\").cast(pl.Utf8)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd6d63-bfbd-4f8a-aec7-8348812f0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineas = transformations(\"TD_LINEAS_INTMOVIL_ITE_VA\",tweak_columns,schema_lineas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1448302f-fcbe-4e1b-8b36-30531120dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca3d112-f685-45a2-bae9-cf6b072d25e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DAG in module airflow.models.dag:\n",
      "\n",
      "class DAG(airflow.utils.log.logging_mixin.LoggingMixin)\n",
      " |  DAG(dag_id: 'str', description: 'str | None' = None, schedule: 'ScheduleArg' = <airflow.utils.types.ArgNotSet object at 0x7fd645771640>, schedule_interval: 'ScheduleIntervalArg' = <airflow.utils.types.ArgNotSet object at 0x7fd645771640>, timetable: 'Timetable | None' = None, start_date: 'datetime | None' = None, end_date: 'datetime | None' = None, full_filepath: 'str | None' = None, template_searchpath: 'str | Iterable[str] | None' = None, template_undefined: 'type[jinja2.StrictUndefined]' = <class 'jinja2.runtime.StrictUndefined'>, user_defined_macros: 'dict | None' = None, user_defined_filters: 'dict | None' = None, default_args: 'dict | None' = None, concurrency: 'int | None' = None, max_active_tasks: 'int' = 16, max_active_runs: 'int' = 16, max_consecutive_failed_dag_runs: 'int' = 0, dagrun_timeout: 'timedelta | None' = None, sla_miss_callback: 'None | SLAMissCallback | list[SLAMissCallback]' = None, default_view: 'str' = 'grid', orientation: 'str' = 'LR', catchup: 'bool' = True, on_success_callback: 'None | DagStateChangeCallback | list[DagStateChangeCallback]' = None, on_failure_callback: 'None | DagStateChangeCallback | list[DagStateChangeCallback]' = None, doc_md: 'str | None' = None, params: 'abc.MutableMapping | None' = None, access_control: 'dict | None' = None, is_paused_upon_creation: 'bool | None' = None, jinja_environment_kwargs: 'dict | None' = None, render_template_as_native_obj: 'bool' = False, tags: 'list[str] | None' = None, owner_links: 'dict[str, str] | None' = None, auto_register: 'bool' = True, fail_stop: 'bool' = False, dag_display_name: 'str | None' = None)\n",
      " |\n",
      " |  A dag (directed acyclic graph) is a collection of tasks with directional dependencies.\n",
      " |\n",
      " |  A dag also has a schedule, a start date and an end date (optional).  For each schedule,\n",
      " |  (say daily or hourly), the DAG needs to run each individual tasks as their dependencies\n",
      " |  are met. Certain tasks have the property of depending on their own past, meaning that\n",
      " |  they can't run until their previous schedule (and upstream tasks) are completed.\n",
      " |\n",
      " |  DAGs essentially act as namespaces for tasks. A task_id can only be\n",
      " |  added once to a DAG.\n",
      " |\n",
      " |  Note that if you plan to use time zones all the dates provided should be pendulum\n",
      " |  dates. See :ref:`timezone_aware_dags`.\n",
      " |\n",
      " |  .. versionadded:: 2.4\n",
      " |      The *schedule* argument to specify either time-based scheduling logic\n",
      " |      (timetable), or dataset-driven triggers.\n",
      " |\n",
      " |  .. deprecated:: 2.4\n",
      " |      The arguments *schedule_interval* and *timetable*. Their functionalities\n",
      " |      are merged into the new *schedule* argument.\n",
      " |\n",
      " |  :param dag_id: The id of the DAG; must consist exclusively of alphanumeric\n",
      " |      characters, dashes, dots and underscores (all ASCII)\n",
      " |  :param description: The description for the DAG to e.g. be shown on the webserver\n",
      " |  :param schedule: Defines the rules according to which DAG runs are scheduled. Can\n",
      " |      accept cron string, timedelta object, Timetable, or list of Dataset objects.\n",
      " |      If this is not provided, the DAG will be set to the default\n",
      " |      schedule ``timedelta(days=1)``. See also :doc:`/howto/timetable`.\n",
      " |  :param start_date: The timestamp from which the scheduler will\n",
      " |      attempt to backfill\n",
      " |  :param end_date: A date beyond which your DAG won't run, leave to None\n",
      " |      for open-ended scheduling\n",
      " |  :param template_searchpath: This list of folders (non-relative)\n",
      " |      defines where jinja will look for your templates. Order matters.\n",
      " |      Note that jinja/airflow includes the path of your DAG file by\n",
      " |      default\n",
      " |  :param template_undefined: Template undefined type.\n",
      " |  :param user_defined_macros: a dictionary of macros that will be exposed\n",
      " |      in your jinja templates. For example, passing ``dict(foo='bar')``\n",
      " |      to this argument allows you to ``{{ foo }}`` in all jinja\n",
      " |      templates related to this DAG. Note that you can pass any\n",
      " |      type of object here.\n",
      " |  :param user_defined_filters: a dictionary of filters that will be exposed\n",
      " |      in your jinja templates. For example, passing\n",
      " |      ``dict(hello=lambda name: 'Hello %s' % name)`` to this argument allows\n",
      " |      you to ``{{ 'world' | hello }}`` in all jinja templates related to\n",
      " |      this DAG.\n",
      " |  :param default_args: A dictionary of default parameters to be used\n",
      " |      as constructor keyword parameters when initialising operators.\n",
      " |      Note that operators have the same hook, and precede those defined\n",
      " |      here, meaning that if your dict contains `'depends_on_past': True`\n",
      " |      here and `'depends_on_past': False` in the operator's call\n",
      " |      `default_args`, the actual value will be `False`.\n",
      " |  :param params: a dictionary of DAG level parameters that are made\n",
      " |      accessible in templates, namespaced under `params`. These\n",
      " |      params can be overridden at the task level.\n",
      " |  :param max_active_tasks: the number of task instances allowed to run\n",
      " |      concurrently\n",
      " |  :param max_active_runs: maximum number of active DAG runs, beyond this\n",
      " |      number of DAG runs in a running state, the scheduler won't create\n",
      " |      new active DAG runs\n",
      " |  :param max_consecutive_failed_dag_runs: (experimental) maximum number of consecutive failed DAG runs,\n",
      " |      beyond this the scheduler will disable the DAG\n",
      " |  :param dagrun_timeout: specify how long a DagRun should be up before\n",
      " |      timing out / failing, so that new DagRuns can be created.\n",
      " |  :param sla_miss_callback: specify a function or list of functions to call when reporting SLA\n",
      " |      timeouts. See :ref:`sla_miss_callback<concepts:sla_miss_callback>` for\n",
      " |      more information about the function signature and parameters that are\n",
      " |      passed to the callback.\n",
      " |  :param default_view: Specify DAG default view (grid, graph, duration,\n",
      " |                                                 gantt, landing_times), default grid\n",
      " |  :param orientation: Specify DAG orientation in graph view (LR, TB, RL, BT), default LR\n",
      " |  :param catchup: Perform scheduler catchup (or only run latest)? Defaults to True\n",
      " |  :param on_failure_callback: A function or list of functions to be called when a DagRun of this dag fails.\n",
      " |      A context dictionary is passed as a single parameter to this function.\n",
      " |  :param on_success_callback: Much like the ``on_failure_callback`` except\n",
      " |      that it is executed when the dag succeeds.\n",
      " |  :param access_control: Specify optional DAG-level actions, e.g.,\n",
      " |      \"{'role1': {'can_read'}, 'role2': {'can_read', 'can_edit', 'can_delete'}}\"\n",
      " |  :param is_paused_upon_creation: Specifies if the dag is paused when created for the first time.\n",
      " |      If the dag exists already, this flag will be ignored. If this optional parameter\n",
      " |      is not specified, the global config setting will be used.\n",
      " |  :param jinja_environment_kwargs: additional configuration options to be passed to Jinja\n",
      " |      ``Environment`` for template rendering\n",
      " |\n",
      " |      **Example**: to avoid Jinja from removing a trailing newline from template strings ::\n",
      " |\n",
      " |          DAG(\n",
      " |              dag_id=\"my-dag\",\n",
      " |              jinja_environment_kwargs={\n",
      " |                  \"keep_trailing_newline\": True,\n",
      " |                  # some other jinja2 Environment options here\n",
      " |              },\n",
      " |          )\n",
      " |\n",
      " |      **See**: `Jinja Environment documentation\n",
      " |      <https://jinja.palletsprojects.com/en/2.11.x/api/#jinja2.Environment>`_\n",
      " |\n",
      " |  :param render_template_as_native_obj: If True, uses a Jinja ``NativeEnvironment``\n",
      " |      to render templates as native Python types. If False, a Jinja\n",
      " |      ``Environment`` is used to render templates as string values.\n",
      " |  :param tags: List of tags to help filtering DAGs in the UI.\n",
      " |  :param owner_links: Dict of owners and their links, that will be clickable on the DAGs view UI.\n",
      " |      Can be used as an HTTP link (for example the link to your Slack channel), or a mailto link.\n",
      " |      e.g: {\"dag_owner\": \"https://airflow.apache.org/\"}\n",
      " |  :param auto_register: Automatically register this DAG when it is used in a ``with`` block\n",
      " |  :param fail_stop: Fails currently running tasks when task in DAG fails.\n",
      " |      **Warning**: A fail stop dag can only have tasks with the default trigger rule (\"all_success\").\n",
      " |      An exception will be thrown if any task in a fail stop dag has a non default trigger rule.\n",
      " |  :param dag_display_name: The display name of the DAG which appears on the UI.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      DAG\n",
      " |      airflow.utils.log.logging_mixin.LoggingMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __deepcopy__(self, memo)\n",
      " |\n",
      " |  __enter__(self)\n",
      " |      # Context Manager -----------------------------------------------\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __exit__(self, _type, _value, _tb)\n",
      " |\n",
      " |  __ge__(self, other) from functools\n",
      " |      Return a >= b.  Computed by @total_ordering from (not a < b).\n",
      " |\n",
      " |  __gt__(self, other) from functools\n",
      " |      Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).\n",
      " |\n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |\n",
      " |  __init__(self, dag_id: 'str', description: 'str | None' = None, schedule: 'ScheduleArg' = <airflow.utils.types.ArgNotSet object at 0x7fd645771640>, schedule_interval: 'ScheduleIntervalArg' = <airflow.utils.types.ArgNotSet object at 0x7fd645771640>, timetable: 'Timetable | None' = None, start_date: 'datetime | None' = None, end_date: 'datetime | None' = None, full_filepath: 'str | None' = None, template_searchpath: 'str | Iterable[str] | None' = None, template_undefined: 'type[jinja2.StrictUndefined]' = <class 'jinja2.runtime.StrictUndefined'>, user_defined_macros: 'dict | None' = None, user_defined_filters: 'dict | None' = None, default_args: 'dict | None' = None, concurrency: 'int | None' = None, max_active_tasks: 'int' = 16, max_active_runs: 'int' = 16, max_consecutive_failed_dag_runs: 'int' = 0, dagrun_timeout: 'timedelta | None' = None, sla_miss_callback: 'None | SLAMissCallback | list[SLAMissCallback]' = None, default_view: 'str' = 'grid', orientation: 'str' = 'LR', catchup: 'bool' = True, on_success_callback: 'None | DagStateChangeCallback | list[DagStateChangeCallback]' = None, on_failure_callback: 'None | DagStateChangeCallback | list[DagStateChangeCallback]' = None, doc_md: 'str | None' = None, params: 'abc.MutableMapping | None' = None, access_control: 'dict | None' = None, is_paused_upon_creation: 'bool | None' = None, jinja_environment_kwargs: 'dict | None' = None, render_template_as_native_obj: 'bool' = False, tags: 'list[str] | None' = None, owner_links: 'dict[str, str] | None' = None, auto_register: 'bool' = True, fail_stop: 'bool' = False, dag_display_name: 'str | None' = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __le__(self, other) from functools\n",
      " |      Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).\n",
      " |\n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |\n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  add_task(self, task: 'Operator') -> 'None'\n",
      " |      Add a task to the DAG.\n",
      " |\n",
      " |      :param task: the task you want to add\n",
      " |\n",
      " |  add_tasks(self, tasks: 'Iterable[Operator]') -> 'None'\n",
      " |      Add a list of tasks to the DAG.\n",
      " |\n",
      " |      :param tasks: a lit of tasks you want to add\n",
      " |\n",
      " |  clear(self, task_ids: 'Collection[str | tuple[str, int]] | None' = None, start_date: 'datetime | None' = None, end_date: 'datetime | None' = None, only_failed: 'bool' = False, only_running: 'bool' = False, confirm_prompt: 'bool' = False, include_subdags: 'bool' = True, include_parentdag: 'bool' = True, dag_run_state: 'DagRunState' = <DagRunState.QUEUED: 'queued'>, dry_run: 'bool' = False, session: 'Session' = None, get_tis: 'bool' = False, recursion_depth: 'int' = 0, max_recursion_depth: 'int | None' = None, dag_bag: 'DagBag | None' = None, exclude_task_ids: 'frozenset[str] | frozenset[tuple[str, int]] | None' = frozenset()) -> 'int | Iterable[TaskInstance]'\n",
      " |      Clear a set of task instances associated with the current dag for a specified date range.\n",
      " |\n",
      " |      :param task_ids: List of task ids or (``task_id``, ``map_index``) tuples to clear\n",
      " |      :param start_date: The minimum execution_date to clear\n",
      " |      :param end_date: The maximum execution_date to clear\n",
      " |      :param only_failed: Only clear failed tasks\n",
      " |      :param only_running: Only clear running tasks.\n",
      " |      :param confirm_prompt: Ask for confirmation\n",
      " |      :param include_subdags: Clear tasks in subdags and clear external tasks\n",
      " |          indicated by ExternalTaskMarker\n",
      " |      :param include_parentdag: Clear tasks in the parent dag of the subdag.\n",
      " |      :param dag_run_state: state to set DagRun to. If set to False, dagrun state will not\n",
      " |          be changed.\n",
      " |      :param dry_run: Find the tasks to clear but don't clear them.\n",
      " |      :param session: The sqlalchemy session to use\n",
      " |      :param dag_bag: The DagBag used to find the dags subdags (Optional)\n",
      " |      :param exclude_task_ids: A set of ``task_id`` or (``task_id``, ``map_index``)\n",
      " |          tuples that should not be cleared\n",
      " |\n",
      " |  cli(self)\n",
      " |      Exposes a CLI specific to this DAG.\n",
      " |\n",
      " |  create_dagrun(self, state: 'DagRunState', execution_date: 'datetime | None' = None, run_id: 'str | None' = None, start_date: 'datetime | None' = None, external_trigger: 'bool | None' = False, conf: 'dict | None' = None, run_type: 'DagRunType | None' = None, session: 'Session' = None, dag_hash: 'str | None' = None, creating_job_id: 'int | None' = None, data_interval: 'tuple[datetime, datetime] | None' = None)\n",
      " |      Create a dag run from this dag including the tasks associated with this dag.\n",
      " |\n",
      " |      Returns the dag run.\n",
      " |\n",
      " |      :param run_id: defines the run id for this dag run\n",
      " |      :param run_type: type of DagRun\n",
      " |      :param execution_date: the execution date of this dag run\n",
      " |      :param state: the state of the dag run\n",
      " |      :param start_date: the date this dag run should be evaluated\n",
      " |      :param external_trigger: whether this dag run is externally triggered\n",
      " |      :param conf: Dict containing configuration/parameters to pass to the DAG\n",
      " |      :param creating_job_id: id of the job creating this DagRun\n",
      " |      :param session: database session\n",
      " |      :param dag_hash: Hash of Serialized DAG\n",
      " |      :param data_interval: Data interval of the DagRun\n",
      " |\n",
      " |  date_range(self, start_date: 'pendulum.DateTime', num: 'int | None' = None, end_date: 'datetime | None' = None) -> 'list[datetime]'\n",
      " |\n",
      " |  following_schedule(self, dttm)\n",
      " |      Calculate the following schedule for this dag in UTC.\n",
      " |\n",
      " |      :param dttm: utc datetime\n",
      " |      :return: utc datetime\n",
      " |\n",
      " |  get_active_runs(self)\n",
      " |      Return a list of dag run execution dates currently running.\n",
      " |\n",
      " |      :return: List of execution dates\n",
      " |\n",
      " |  get_concurrency_reached(self, session=None) -> 'bool'\n",
      " |      Return a boolean indicating whether the max_active_tasks limit for this DAG has been reached.\n",
      " |\n",
      " |  get_dagrun(self, execution_date: 'datetime | None' = None, run_id: 'str | None' = None, session: 'Session' = None) -> 'DagRun | DagRunPydantic'\n",
      " |\n",
      " |  get_dagruns_between(self, start_date, end_date, session=None)\n",
      " |      Return the list of dag runs between start_date (inclusive) and end_date (inclusive).\n",
      " |\n",
      " |      :param start_date: The starting execution date of the DagRun to find.\n",
      " |      :param end_date: The ending execution date of the DagRun to find.\n",
      " |      :param session:\n",
      " |      :return: The list of DagRuns found.\n",
      " |\n",
      " |  get_default_view(self)\n",
      " |      Allow backward compatible jinja2 templates.\n",
      " |\n",
      " |  get_doc_md(self, doc_md: 'str | None') -> 'str | None'\n",
      " |\n",
      " |  get_edge_info(self, upstream_task_id: 'str', downstream_task_id: 'str') -> 'EdgeInfoType'\n",
      " |      Return edge information for the given pair of tasks or an empty edge if there is no information.\n",
      " |\n",
      " |  get_is_active(self, session=None) -> 'None'\n",
      " |      Return a boolean indicating whether this DAG is active.\n",
      " |\n",
      " |  get_is_paused(self, session=None) -> 'None'\n",
      " |      Return a boolean indicating whether this DAG is paused.\n",
      " |\n",
      " |  get_last_dagrun(self, session=None, include_externally_triggered=False)\n",
      " |\n",
      " |  get_latest_execution_date(self, session: 'Session' = None) -> 'pendulum.DateTime | None'\n",
      " |      Return the latest date for which at least one dag run exists.\n",
      " |\n",
      " |  get_next_data_interval(self, dag_model: 'DagModel') -> 'DataInterval | None'\n",
      " |      Get the data interval of the next scheduled run.\n",
      " |\n",
      " |      For compatibility, this method infers the data interval from the DAG's\n",
      " |      schedule if the run does not have an explicit one set, which is possible\n",
      " |      for runs created prior to AIP-39.\n",
      " |\n",
      " |      This function is private to Airflow core and should not be depended on as a\n",
      " |      part of the Python API.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  get_num_active_runs(self, external_trigger=None, only_running=True, session=None)\n",
      " |      Return the number of active \"running\" dag runs.\n",
      " |\n",
      " |      :param external_trigger: True for externally triggered active dag runs\n",
      " |      :param session:\n",
      " |      :return: number greater than 0 for active dag runs\n",
      " |\n",
      " |  get_run_data_interval(self, run: 'DagRun | DagRunPydantic') -> 'DataInterval'\n",
      " |      Get the data interval of this run.\n",
      " |\n",
      " |      For compatibility, this method infers the data interval from the DAG's\n",
      " |      schedule if the run does not have an explicit one set, which is possible for\n",
      " |      runs created prior to AIP-39.\n",
      " |\n",
      " |      This function is private to Airflow core and should not be depended on as a\n",
      " |      part of the Python API.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  get_run_dates(self, start_date, end_date=None) -> 'list'\n",
      " |      Return a list of dates between the interval received as parameter using this dag's schedule interval.\n",
      " |\n",
      " |      Returned dates can be used for execution dates.\n",
      " |\n",
      " |      :param start_date: The start date of the interval.\n",
      " |      :param end_date: The end date of the interval. Defaults to ``timezone.utcnow()``.\n",
      " |      :return: A list of dates within the interval following the dag's schedule.\n",
      " |\n",
      " |  get_task(self, task_id: 'str', include_subdags: 'bool' = False) -> 'Operator'\n",
      " |\n",
      " |  get_task_instances(self, start_date: 'datetime | None' = None, end_date: 'datetime | None' = None, state: 'list[TaskInstanceState] | None' = None, session: 'Session' = None) -> 'list[TaskInstance]'\n",
      " |\n",
      " |  get_task_instances_before(self, base_date: 'datetime', num: 'int', *, session: 'Session' = None) -> 'list[TaskInstance]'\n",
      " |      Get ``num`` task instances before (including) ``base_date``.\n",
      " |\n",
      " |      The returned list may contain exactly ``num`` task instances\n",
      " |      corresponding to any DagRunType. It can have less if there are\n",
      " |      less than ``num`` scheduled DAG runs before ``base_date``.\n",
      " |\n",
      " |  get_template_env(self, *, force_sandboxed: 'bool' = False) -> 'jinja2.Environment'\n",
      " |      Build a Jinja2 environment.\n",
      " |\n",
      " |  get_tree_view(self) -> 'str'\n",
      " |      Return an ASCII tree representation of the DAG.\n",
      " |\n",
      " |  handle_callback(self, dagrun: 'DagRun', success=True, reason=None, session=None)\n",
      " |      Triggers on_failure_callback or on_success_callback as appropriate.\n",
      " |\n",
      " |      This method gets the context of a single TaskInstance part of this DagRun\n",
      " |      and passes that to the callable along with a 'reason', primarily to\n",
      " |      differentiate DagRun failures.\n",
      " |\n",
      " |      .. note: The logs end up in\n",
      " |          ``$AIRFLOW_HOME/logs/scheduler/latest/PROJECT/DAG_FILE.py.log``\n",
      " |\n",
      " |      :param dagrun: DagRun object\n",
      " |      :param success: Flag to specify if failure or success callback should be called\n",
      " |      :param reason: Completion reason\n",
      " |      :param session: Database session\n",
      " |\n",
      " |  has_dag_runs(self, session=None, include_externally_triggered=True) -> 'bool'\n",
      " |\n",
      " |  has_task(self, task_id: 'str')\n",
      " |\n",
      " |  has_task_group(self, task_group_id: 'str') -> 'bool'\n",
      " |\n",
      " |  infer_automated_data_interval(self, logical_date: 'datetime') -> 'DataInterval'\n",
      " |      Infer a data interval for a run against this DAG.\n",
      " |\n",
      " |      This method is used to bridge runs created prior to AIP-39\n",
      " |      implementation, which do not have an explicit data interval. Therefore,\n",
      " |      this method only considers ``schedule_interval`` values valid prior to\n",
      " |      Airflow 2.2.\n",
      " |\n",
      " |      DO NOT call this method if there is a known data interval.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  is_fixed_time_schedule(self)\n",
      " |      Figures out if the schedule has a fixed time (e.g. 3 AM every day).\n",
      " |\n",
      " |      Detection is done by \"peeking\" the next two cron trigger time; if the\n",
      " |      two times have the same minute and hour value, the schedule is fixed,\n",
      " |      and we *don't* need to perform the DST fix.\n",
      " |\n",
      " |      This assumes DST happens on whole minute changes (e.g. 12:59 -> 12:00).\n",
      " |\n",
      " |      Do not try to understand what this actually means. It is old logic that\n",
      " |      should not be used anywhere.\n",
      " |\n",
      " |  iter_dagrun_infos_between(self, earliest: 'pendulum.DateTime | None', latest: 'pendulum.DateTime', *, align: 'bool' = True) -> 'Iterable[DagRunInfo]'\n",
      " |      Yield DagRunInfo using this DAG's timetable between given interval.\n",
      " |\n",
      " |      DagRunInfo instances yielded if their ``logical_date`` is not earlier\n",
      " |      than ``earliest``, nor later than ``latest``. The instances are ordered\n",
      " |      by their ``logical_date`` from earliest to latest.\n",
      " |\n",
      " |      If ``align`` is ``False``, the first run will happen immediately on\n",
      " |      ``earliest``, even if it does not fall on the logical timetable schedule.\n",
      " |      The default is ``True``, but subdags will ignore this value and always\n",
      " |      behave as if this is set to ``False`` for backward compatibility.\n",
      " |\n",
      " |      Example: A DAG is scheduled to run every midnight (``0 0 * * *``). If\n",
      " |      ``earliest`` is ``2021-06-03 23:00:00``, the first DagRunInfo would be\n",
      " |      ``2021-06-03 23:00:00`` if ``align=False``, and ``2021-06-04 00:00:00``\n",
      " |      if ``align=True``.\n",
      " |\n",
      " |  iter_invalid_owner_links(self) -> 'Iterator[tuple[str, str]]'\n",
      " |      Parse a given link, and verifies if it's a valid URL, or a 'mailto' link.\n",
      " |\n",
      " |      Returns an iterator of invalid (owner, link) pairs.\n",
      " |\n",
      " |  next_dagrun_after_date(self, date_last_automated_dagrun: 'pendulum.DateTime | None')\n",
      " |\n",
      " |  next_dagrun_info(self, last_automated_dagrun: 'None | datetime | DataInterval', *, restricted: 'bool' = True) -> 'DagRunInfo | None'\n",
      " |      Get information about the next DagRun of this dag after ``date_last_automated_dagrun``.\n",
      " |\n",
      " |      This calculates what time interval the next DagRun should operate on\n",
      " |      (its execution date) and when it can be scheduled, according to the\n",
      " |      dag's timetable, start_date, end_date, etc. This doesn't check max\n",
      " |      active run or any other \"max_active_tasks\" type limits, but only\n",
      " |      performs calculations based on the various date and interval fields of\n",
      " |      this dag and its tasks.\n",
      " |\n",
      " |      :param last_automated_dagrun: The ``max(execution_date)`` of\n",
      " |          existing \"automated\" DagRuns for this dag (scheduled or backfill,\n",
      " |          but not manual).\n",
      " |      :param restricted: If set to *False* (default is *True*), ignore\n",
      " |          ``start_date``, ``end_date``, and ``catchup`` specified on the DAG\n",
      " |          or tasks.\n",
      " |      :return: DagRunInfo of the next dagrun, or None if a dagrun is not\n",
      " |          going to be scheduled.\n",
      " |\n",
      " |  normalize_schedule(self, dttm)\n",
      " |\n",
      " |  param(self, name: 'str', default: 'Any' = <airflow.utils.types.ArgNotSet object at 0x7fd645771640>) -> 'DagParam'\n",
      " |      Return a DagParam object for current dag.\n",
      " |\n",
      " |      :param name: dag parameter name.\n",
      " |      :param default: fallback value for dag parameter.\n",
      " |      :return: DagParam instance for specified name and current dag.\n",
      " |\n",
      " |  partial_subset(self, task_ids_or_regex: 'str | Pattern | Iterable[str]', include_downstream=False, include_upstream=True, include_direct_upstream=False)\n",
      " |      Return a subset of the current dag based on regex matching one or more tasks.\n",
      " |\n",
      " |      Returns a subset of the current dag as a deep copy of the current dag\n",
      " |      based on a regex that should match one or many tasks, and includes\n",
      " |      upstream and downstream neighbours based on the flag passed.\n",
      " |\n",
      " |      :param task_ids_or_regex: Either a list of task_ids, or a regex to\n",
      " |          match against task ids (as a string, or compiled regex pattern).\n",
      " |      :param include_downstream: Include all downstream tasks of matched\n",
      " |          tasks, in addition to matched tasks.\n",
      " |      :param include_upstream: Include all upstream tasks of matched tasks,\n",
      " |          in addition to matched tasks.\n",
      " |      :param include_direct_upstream: Include all tasks directly upstream of matched\n",
      " |          and downstream (if include_downstream = True) tasks\n",
      " |\n",
      " |  pickle(self, session=None) -> 'DagPickle'\n",
      " |\n",
      " |  pickle_info(self)\n",
      " |\n",
      " |  previous_schedule(self, dttm)\n",
      " |\n",
      " |  resolve_template_files(self)\n",
      " |\n",
      " |  run(self, start_date=None, end_date=None, mark_success=False, local=False, executor=None, donot_pickle=True, ignore_task_deps=False, ignore_first_depends_on_past=True, pool=None, delay_on_limit_secs=1.0, verbose=False, conf=None, rerun_failed_tasks=False, run_backwards=False, run_at_least_once=False, continue_on_failures=False, disable_retry=False)\n",
      " |      Run the DAG.\n",
      " |\n",
      " |      :param start_date: the start date of the range to run\n",
      " |      :param end_date: the end date of the range to run\n",
      " |      :param mark_success: True to mark jobs as succeeded without running them\n",
      " |      :param local: True to run the tasks using the LocalExecutor\n",
      " |      :param executor: The executor instance to run the tasks\n",
      " |      :param donot_pickle: True to avoid pickling DAG object and send to workers\n",
      " |      :param ignore_task_deps: True to skip upstream tasks\n",
      " |      :param ignore_first_depends_on_past: True to ignore depends_on_past\n",
      " |          dependencies for the first set of tasks only\n",
      " |      :param pool: Resource pool to use\n",
      " |      :param delay_on_limit_secs: Time in seconds to wait before next attempt to run\n",
      " |          dag run when max_active_runs limit has been reached\n",
      " |      :param verbose: Make logging output more verbose\n",
      " |      :param conf: user defined dictionary passed from CLI\n",
      " |      :param rerun_failed_tasks:\n",
      " |      :param run_backwards:\n",
      " |      :param run_at_least_once: If true, always run the DAG at least once even\n",
      " |          if no logical run exists within the time range.\n",
      " |\n",
      " |  set_dag_runs_state(self, state: 'DagRunState' = <DagRunState.RUNNING: 'running'>, session: 'Session' = None, start_date: 'datetime | None' = None, end_date: 'datetime | None' = None, dag_ids: 'list[str] | None' = None) -> 'None'\n",
      " |\n",
      " |  set_dependency(self, upstream_task_id, downstream_task_id)\n",
      " |      Set dependency between two tasks that already have been added to the DAG using add_task().\n",
      " |\n",
      " |  set_edge_info(self, upstream_task_id: 'str', downstream_task_id: 'str', info: 'EdgeInfoType')\n",
      " |      Set the given edge information on the DAG.\n",
      " |\n",
      " |      Note that this will overwrite, rather than merge with, existing info.\n",
      " |\n",
      " |  set_task_group_state(self, *, group_id: 'str', execution_date: 'datetime | None' = None, run_id: 'str | None' = None, state: 'TaskInstanceState', upstream: 'bool' = False, downstream: 'bool' = False, future: 'bool' = False, past: 'bool' = False, commit: 'bool' = True, session: 'Session' = None) -> 'list[TaskInstance]'\n",
      " |      Set TaskGroup to the given state and clear downstream tasks in failed or upstream_failed state.\n",
      " |\n",
      " |      :param group_id: The group_id of the TaskGroup\n",
      " |      :param execution_date: Execution date of the TaskInstance\n",
      " |      :param run_id: The run_id of the TaskInstance\n",
      " |      :param state: State to set the TaskInstance to\n",
      " |      :param upstream: Include all upstream tasks of the given task_id\n",
      " |      :param downstream: Include all downstream tasks of the given task_id\n",
      " |      :param future: Include all future TaskInstances of the given task_id\n",
      " |      :param commit: Commit changes\n",
      " |      :param past: Include all past TaskInstances of the given task_id\n",
      " |      :param session: new session\n",
      " |\n",
      " |  set_task_instance_state(self, *, task_id: 'str', map_indexes: 'Collection[int] | None' = None, execution_date: 'datetime | None' = None, run_id: 'str | None' = None, state: 'TaskInstanceState', upstream: 'bool' = False, downstream: 'bool' = False, future: 'bool' = False, past: 'bool' = False, commit: 'bool' = True, session=None) -> 'list[TaskInstance]'\n",
      " |      Set the state of a TaskInstance and clear downstream tasks in failed or upstream_failed state.\n",
      " |\n",
      " |      :param task_id: Task ID of the TaskInstance\n",
      " |      :param map_indexes: Only set TaskInstance if its map_index matches.\n",
      " |          If None (default), all mapped TaskInstances of the task are set.\n",
      " |      :param execution_date: Execution date of the TaskInstance\n",
      " |      :param run_id: The run_id of the TaskInstance\n",
      " |      :param state: State to set the TaskInstance to\n",
      " |      :param upstream: Include all upstream tasks of the given task_id\n",
      " |      :param downstream: Include all downstream tasks of the given task_id\n",
      " |      :param future: Include all future TaskInstances of the given task_id\n",
      " |      :param commit: Commit changes\n",
      " |      :param past: Include all past TaskInstances of the given task_id\n",
      " |\n",
      " |  simplify_dataset_expression(self, dataset_expression) -> 'dict | None'\n",
      " |      Simplifies a nested dataset expression into a 'any' or 'all' format with URIs.\n",
      " |\n",
      " |  sub_dag(self, *args, **kwargs)\n",
      " |      Use `airflow.models.DAG.partial_subset`, this method is deprecated.\n",
      " |\n",
      " |  sync_to_db(self, processor_subdir: 'str | None' = None, session=None)\n",
      " |      Save attributes about this DAG to the DB.\n",
      " |\n",
      " |      Note that this method can be called for both DAGs and SubDAGs. A SubDag is actually a SubDagOperator.\n",
      " |\n",
      " |      :return: None\n",
      " |\n",
      " |  task_group_dict = <functools.cached_property object>\n",
      " |  test(self, execution_date: 'datetime | None' = None, run_conf: 'dict[str, Any] | None' = None, conn_file_path: 'str | None' = None, variable_file_path: 'str | None' = None, session: 'Session' = None) -> 'DagRun'\n",
      " |      Execute one single DagRun for a given DAG and execution date.\n",
      " |\n",
      " |      :param execution_date: execution date for the DAG run\n",
      " |      :param run_conf: configuration to pass to newly created dagrun\n",
      " |      :param conn_file_path: file path to a connection file in either yaml or json\n",
      " |      :param variable_file_path: file path to a variable file in either yaml or json\n",
      " |      :param session: database connection (optional)\n",
      " |\n",
      " |  topological_sort(self, include_subdag_tasks: 'bool' = False)\n",
      " |      Sorts tasks in topographical order, such that a task comes after any of its upstream dependencies.\n",
      " |\n",
      " |      Deprecated in place of ``task_group.topological_sort``\n",
      " |\n",
      " |  tree_view(self) -> 'None'\n",
      " |      Print an ASCII tree representation of the DAG.\n",
      " |\n",
      " |  validate(self)\n",
      " |      Validate the DAG has a coherent setup.\n",
      " |\n",
      " |      This is called by the DAG bag before bagging the DAG.\n",
      " |\n",
      " |  validate_schedule_and_params(self)\n",
      " |      Validate Param values when the DAG has schedule defined.\n",
      " |\n",
      " |      Raise exception if there are any Params which can not be resolved by their schema definition.\n",
      " |\n",
      " |  validate_setup_teardown(self)\n",
      " |      Validate that setup and teardown tasks are configured properly.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  bulk_sync_to_db(dags: 'Collection[DAG]', session=None)\n",
      " |      Use `airflow.models.DAG.bulk_write_to_db`, this method is deprecated.\n",
      " |\n",
      " |  bulk_write_to_db(dags: 'Collection[DAG]', processor_subdir: 'str | None' = None, session=None)\n",
      " |      Ensure the DagModel rows for the given dags are up-to-date in the dag table in the DB.\n",
      " |\n",
      " |      Note that this method can be called for both DAGs and SubDAGs. A SubDag is actually a SubDagOperator.\n",
      " |\n",
      " |      :param dags: the DAG objects to save to the DB\n",
      " |      :return: None\n",
      " |\n",
      " |  clear_dags(dags, start_date=None, end_date=None, only_failed=False, only_running=False, confirm_prompt=False, include_subdags=True, include_parentdag=False, dag_run_state=<DagRunState.QUEUED: 'queued'>, dry_run=False)\n",
      " |\n",
      " |  execute_callback(callbacks: 'list[Callable] | None', context: 'Context | None', dag_id: 'str')\n",
      " |      Triggers the callbacks with the given context.\n",
      " |\n",
      " |      :param callbacks: List of callbacks to call\n",
      " |      :param context: Context to pass to all callbacks\n",
      " |      :param dag_id: The dag_id of the DAG to find.\n",
      " |\n",
      " |  get_serialized_fields()\n",
      " |      Stringified DAGs and operators contain exactly these fields.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  deactivate_stale_dags(expiration_date, session=None)\n",
      " |      Deactivate any DAGs that were last touched by the scheduler before the expiration date.\n",
      " |\n",
      " |      These DAGs were likely deleted.\n",
      " |\n",
      " |      :param expiration_date: set inactive DAGs that were touched before this time\n",
      " |      :return: None\n",
      " |\n",
      " |  deactivate_unknown_dags(active_dag_ids, session=None)\n",
      " |      Given a list of known DAGs, deactivate any other DAGs that are marked as active in the ORM.\n",
      " |\n",
      " |      :param active_dag_ids: list of DAG IDs that are active\n",
      " |      :return: None\n",
      " |\n",
      " |  fetch_callback(dag: 'DAG', dag_run_id: 'str', success: 'bool' = True, reason: 'str | None' = None, *, session: 'Session' = None) -> 'tuple[list[TaskStateChangeCallback], Context] | None'\n",
      " |      Fetch the appropriate callbacks depending on the value of success.\n",
      " |\n",
      " |      This method gets the context of a single TaskInstance part of this DagRun and returns it along\n",
      " |      the list of callbacks.\n",
      " |\n",
      " |      :param dag: DAG object\n",
      " |      :param dag_run_id: The DAG run ID\n",
      " |      :param success: Flag to specify if failure or success callback should be called\n",
      " |      :param reason: Completion reason\n",
      " |      :param session: Database session\n",
      " |\n",
      " |  fetch_dagrun(dag_id: 'str', execution_date: 'datetime | None' = None, run_id: 'str | None' = None, session: 'Session' = None) -> 'DagRun | DagRunPydantic'\n",
      " |      Return the dag run for a given execution date or run_id if it exists, otherwise none.\n",
      " |\n",
      " |      :param dag_id: The dag_id of the DAG to find.\n",
      " |      :param execution_date: The execution date of the DagRun to find.\n",
      " |      :param run_id: The run_id of the DagRun to find.\n",
      " |      :param session:\n",
      " |      :return: The DagRun if found, otherwise None.\n",
      " |\n",
      " |  get_num_task_instances(dag_id, run_id=None, task_ids=None, states=None, session=None) -> 'int'\n",
      " |      Return the number of task instances in the given DAG.\n",
      " |\n",
      " |      :param session: ORM session\n",
      " |      :param dag_id: ID of the DAG to get the task concurrency of\n",
      " |      :param run_id: ID of the DAG run to get the task concurrency of\n",
      " |      :param task_ids: A list of valid task IDs for the given DAG\n",
      " |      :param states: A list of states to filter by if supplied\n",
      " |      :return: The number of running tasks\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  allow_future_exec_dates\n",
      " |\n",
      " |  concurrency_reached\n",
      " |      Use `airflow.models.DAG.get_concurrency_reached`, this attribute is deprecated.\n",
      " |\n",
      " |  dag_display_name\n",
      " |\n",
      " |  default_view\n",
      " |\n",
      " |  description\n",
      " |\n",
      " |  filepath\n",
      " |      Relative file path to the DAG.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  folder\n",
      " |      Folder location of where the DAG object is instantiated.\n",
      " |\n",
      " |  is_paused\n",
      " |      Use `airflow.models.DAG.get_is_paused`, this attribute is deprecated.\n",
      " |\n",
      " |  is_subdag\n",
      " |\n",
      " |  latest_execution_date\n",
      " |      Use `airflow.models.DAG.get_latest_execution_date`, this attribute is deprecated.\n",
      " |\n",
      " |  leaves\n",
      " |      Return nodes with no children. These are last to execute and are called leaves or leaf nodes.\n",
      " |\n",
      " |  normalized_schedule_interval\n",
      " |\n",
      " |  owner\n",
      " |      Return list of all owners found in DAG tasks.\n",
      " |\n",
      " |      :return: Comma separated list of owners in DAG tasks\n",
      " |\n",
      " |  relative_fileloc\n",
      " |      File location of the importable dag 'file' relative to the configured DAGs folder.\n",
      " |\n",
      " |  roots\n",
      " |      Return nodes with no parents. These are first to execute and are called roots or root nodes.\n",
      " |\n",
      " |  subdags\n",
      " |      Return a list of the subdag objects associated to this DAG.\n",
      " |\n",
      " |  task\n",
      " |\n",
      " |  task_group\n",
      " |\n",
      " |  task_ids\n",
      " |\n",
      " |  tasks_upstream_of_teardowns\n",
      " |\n",
      " |  teardowns\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  access_control\n",
      " |\n",
      " |  concurrency\n",
      " |\n",
      " |  dag_id\n",
      " |\n",
      " |  full_filepath\n",
      " |      Full file path to the DAG.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  max_active_tasks\n",
      " |\n",
      " |  pickle_id\n",
      " |\n",
      " |  tasks\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_DAG__serialized_fields': 'frozenset[str] | None',...\n",
      " |\n",
      " |  parent_dag = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from airflow.utils.log.logging_mixin.LoggingMixin:\n",
      " |\n",
      " |  logger() -> 'Logger'\n",
      " |      Return a logger.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from airflow.utils.log.logging_mixin.LoggingMixin:\n",
      " |\n",
      " |  log\n",
      " |      Return a logger.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from airflow.utils.log.logging_mixin.LoggingMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8da9ff-f208-4c06-88fe-e7a5035cd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.decorators import dag, task\n",
    "from airflow import tasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b923c90e-2450-4eb7-8e00-8bd55990fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TaskDecoratorCollection in module airflow.decorators object:\n",
      "\n",
      "class TaskDecoratorCollection(builtins.object)\n",
      " |  Implementation to provide the ``@task`` syntax.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __getattr__(self, name: 'str') -> 'TaskDecorator'\n",
      " |      Dynamically get provider-registered task decorators, e.g. ``@task.docker``.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __call__ = python_task(python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.python\n",
      " |      Wrap a function into an Airflow operator.\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: If set to True, the decorated function's return value will be unrolled to\n",
      " |          multiple XCom values. Dict will unroll to XCom values with its keys as XCom keys. Defaults to False.\n",
      " |\n",
      " |  bash = bash_task(python_callable: 'Callable | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.bash\n",
      " |      Wrap a function into a BashOperator.\n",
      " |\n",
      " |      Accepts kwargs for operator kwargs. Can be reused in a single DAG. This function is only used only used\n",
      " |      during type checking or auto-completion.\n",
      " |\n",
      " |      :param python_callable: Function to decorate.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  branch = branch_task(python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.branch_python\n",
      " |      Wrap a python function into a BranchPythonOperator.\n",
      " |\n",
      " |      For more information on how to use this operator, take a look at the guide:\n",
      " |      :ref:`concepts:branching`\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: if set, function return value will be\n",
      " |          unrolled to multiple XCom values. Dict will unroll to xcom values with keys as XCom keys.\n",
      " |          Defaults to False.\n",
      " |\n",
      " |  branch_external_python = branch_external_python_task(python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.branch_external_python\n",
      " |      Wrap a python function into a BranchExternalPythonOperator.\n",
      " |\n",
      " |      For more information on how to use this operator, take a look at the guide:\n",
      " |      :ref:`concepts:branching`\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: if set, function return value will be\n",
      " |          unrolled to multiple XCom values. Dict will unroll to xcom values with keys as XCom keys.\n",
      " |          Defaults to False.\n",
      " |\n",
      " |  branch_virtualenv = branch_virtualenv_task(python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.branch_virtualenv\n",
      " |      Wrap a python function into a BranchPythonVirtualenvOperator.\n",
      " |\n",
      " |      For more information on how to use this operator, take a look at the guide:\n",
      " |      :ref:`concepts:branching`\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: if set, function return value will be\n",
      " |          unrolled to multiple XCom values. Dict will unroll to xcom values with keys as XCom keys.\n",
      " |          Defaults to False.\n",
      " |\n",
      " |  external_python = external_python_task(python: 'str | None' = None, python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.external_python\n",
      " |      Wrap a callable into an Airflow operator to run via a Python virtual environment.\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      This function is only used during type checking or auto-completion.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |      :param python: Full path string (file-system specific) that points to a Python binary inside\n",
      " |          a virtualenv that should be used (in ``VENV/bin`` folder). Should be absolute path\n",
      " |          (so usually start with \"/\" or \"X:/\" depending on the filesystem/os used).\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: If set to True, the decorated function's return value will be unrolled to\n",
      " |          multiple XCom values. Dict will unroll to XCom values with its keys as XCom keys.\n",
      " |          Defaults to False.\n",
      " |\n",
      " |  python = python_task(python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.python\n",
      " |      Wrap a function into an Airflow operator.\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: If set to True, the decorated function's return value will be unrolled to\n",
      " |          multiple XCom values. Dict will unroll to XCom values with its keys as XCom keys. Defaults to False.\n",
      " |\n",
      " |  sensor = sensor_task(python_callable: 'Callable | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.sensor\n",
      " |      Wrap a function into an Airflow operator.\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |      :param python_callable: Function to decorate\n",
      " |\n",
      " |  short_circuit = short_circuit_task(python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.short_circuit\n",
      " |      Wrap a function into an ShortCircuitOperator.\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      This function is only used only used during type checking or auto-completion.\n",
      " |\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: If set to True, the decorated function's return value will be unrolled to\n",
      " |          multiple XCom values. Dict will unroll to XCom values with its keys as XCom keys. Defaults to False.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |  virtualenv = virtualenv_task(python_callable: 'Callable | None' = None, multiple_outputs: 'bool | None' = None, **kwargs) -> 'TaskDecorator' from airflow.decorators.python_virtualenv\n",
      " |      Wrap a callable into an Airflow operator to run via a Python virtual environment.\n",
      " |\n",
      " |      Accepts kwargs for operator kwarg. Can be reused in a single DAG.\n",
      " |\n",
      " |      This function is only used only used during type checking or auto-completion.\n",
      " |\n",
      " |      :meta private:\n",
      " |\n",
      " |      :param python_callable: Function to decorate\n",
      " |      :param multiple_outputs: If set to True, the decorated function's return value will be unrolled to\n",
      " |          multiple XCom values. Dict will unroll to XCom values with its keys as XCom keys.\n",
      " |          Defaults to False.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'__call__': 'Any'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35d374-683e-44e6-96f8-6a765fa87286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.decorators import task\n",
    "\n",
    "with DAG(dag_id=\"prueba\",schedule=None) as dag:\n",
    "    @task(task_id=\"primera task\", task_display_name=\"primera task\")\n",
    "    def task_1():\n",
    "        return \"esta es la primera prueba\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
